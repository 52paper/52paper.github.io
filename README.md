### Guideline:
- paper reading讲解的时候要深入浅出，确保自己看懂了，再用通俗的话讲出来。关键是把文章工作讲清楚，motivation，方法部分，实验是否支撑，该工作的优点和缺点，对你个人工作的启发。每部分大概2~3页slides即可。最重要的是后面两部分，需要你自己对工作批判性的阅读。
- 时间暂定是周3晚上晚饭后。 如果人不齐的话提前告知，视情况再确定时间。
- presenter务必每周1发出分享的paper，并且update到 https://52paper.github.io/ 。
- attender希望都能够提前把分享的paper至少通读一遍，积极提出问题及参与discussion。

### 2018/6/20 GAN
- xiaojiang's questions, hope we could have agreements on these three points, and output some reports:
  1. Why Seq2seq is better than the previous language model methods in generating language sequence. Why GAN is better than standard Seq2seq?
  2. GAN has been successfully appllied to many new image tasks, such as image generation. What are the best tasks of GAN for text?
  3. Why GAN has no break-through on text yet? All possible reasons.
- cd [[slide]](./20180620_jcykcai.pdf): Implement Adversarial Training for Text Generation (motivations and technologies)
- gaojun: 
  - EMNLP2017 [Neural Response Generation via GAN with an Approximate Embedding Layer∗](http://aclweb.org/anthology/D17-1065)
  - IJCAI2018 [Commonsense Knowledge Aware Conversation Generation with Graph Attention](http://coai.cs.tsinghua.edu.cn/hml/media/files/2018_commonsense_ZhouHao_3_TYVQ7Iq.pdf)
- yahui [[slide]](./20180620_amosyhliu.pdf)
  - EMNLP2017 [Adversarial Learning for Neural Dialogue Generation](https://nlp.stanford.edu/pubs/li2017adversarial.pdf)
  - ICLR2018 [MaskGAN: Better Text Generation via Filling in the __ ](https://arxiv.org/pdf/1801.07736.pdf)
- biwei 
  - ICML2017 [Adversarial Feature Matching for Text Generation](https://arxiv.org/pdf/1706.03850.pdf)
  - ICML2017 [Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control](https://arxiv.org/abs/1611.02796)
