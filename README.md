### Guideline:
- paper reading讲解的时候要深入浅出，确保自己看懂了，再用通俗的话讲出来。关键是把文章工作讲清楚，motivation，方法部分，实验是否支撑，该工作的优点和缺点，对你个人工作的启发。每部分大概2~3页slides即可。最重要的是后面两部分，需要你自己对工作批判性的阅读。
- 时间暂定是周4下午。 如果人不齐的话提前告知，视情况再确定时间。
- 分享的同学务必提前告知大家分享的论文，并在分享前update paper信息及slides到 [52paper.github.io](https://52paper.github.io/)；新人权限开通请联系jamgao。
- 参与者希望都能够提前把分享的paper进行相关背景的了解，积极提出问题及参与讨论。

### 2019/06/28

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|jcykcai|Rethinking the generation orders of sequence|[[slide]](./20190628-jcykcai.pdf)|-|
|jcykcai|ICML2019 [Insertion Transformer: Flexible Sequence Generation via Insertion Operations](https://arxiv.org/pdf/1902.03249.pdf)|-|-|
|-|ICML2019 [Non-Monotonic Sequential Text Generation](https://arxiv.org/pdf/1902.02192.pdf)|-|-|
|-|arxiv19 [Insertion-based Decoding with automatically Inferred Generation Order](https://arxiv.org/pdf/1902.01370.pdf)|-|-|
|-|EMNLP18 [The Importance of Generation Order in Language Modeling](https://aclweb.org/anthology/D18-1324)|-|-|
|-|arxiv19 [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)|-|-|

### 2019/06/17

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|gaojun|[The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation](https://www.aclweb.org/anthology/P18-1008)|-|-|
|-|[How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures](https://aclweb.org/anthology/P18-1167)|-|-|
|-|[Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures](https://aclweb.org/anthology/D18-1458)|-|-|
|-|[Argument Generation with Retrieval, Planning, and Realization](https://arxiv.org/abs/1906.03717)|-|-|

### 2019/05/30

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|jiachendu|ICLR 2019 [LEARNING TO REPRESENT EDITS](https://arxiv.org/pdf/1810.13337.pdf)|[[slide]](./20190530-jiachendu-edit.pdf)|-|
|-|[Text Infilling](https://arxiv.org/pdf/1901.00158.pdf)|-|-|
|-|[TIGS: An Inference Algorithm for Text Infilling with Gradient Search](https://arxiv.org/pdf/1905.10752v1.pdf)|-|-|


### 2019/05/09

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|lixin| [The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751.pdf)|[[slide]](./20190509_lixin.pdf)|-|

### 2019/04/24

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|evanyfgao(高一帆)| [Reasoning in Multi-hop Reading Comprehension](./20190425-evanyfgao-reasoning.pdf)|[[slide]](./20190425-evanyfgao-reasoning.pdf)|-|


### 2019/04/11

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|royrong(荣钰)| [Representation Learning on Graphs](./20190411_rongyu.pdf)|[[slide]](./20190411_rongyu.pdf)|-|


### 2019/04/04

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|jcykcai|AAAI17 [Mechanism-Aware Neural Machine for Dialogue Response Generation](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14471/14267)|[[slide]](./20190404_jcykcai.pdf)|-|
|-|ACL18 [Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation](https://www.aclweb.org/anthology/P18-1101)|-|-|
|-|EMNLP18 [Learning Neural Templates for Text Generation](https://aclweb.org/anthology/D18-1356)|-|-|


### 2019/03/28

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|yxsu|TACL2018 [Polite Dialogue Generation Without Parallel Data](https://arxiv.org/abs/1805.03162)|[[slide]](./20190328_yxsu.pdf)|-|


### 2019/03/21

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|gaojun|NIPS2018 [Content preserving text generation with attribute controls](http://papers.nips.cc/paper/7757-content-preserving-text-generation-with-attribute-controls.pdf)|[[slide]](./20190321_gaojun.pdf)|-|
|hongyining|EMNLP2017 [Challenges in Data-to-Document Generation ](https://arxiv.org/pdf/1707.08052.pdf) |[[slide]](./20190321_yining.pdf)|-|
|-| [Data-to-Text Generation with Content Selection and Planning](https://arxiv.org/pdf/1809.00582.pdf)|-|-|

### 2019/01/18

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|zhuqile|ICLR2019 [Recent Advances in Autoencoder-Based Representation Learning](https://arxiv.org/abs/1812.05069)|[[slide]](./20190118_qilezhu.pdf)|-|
|jiangtongli|ICLR2019 [Pay Less Attention with Lightweight and Dynamic Convolutions](https://openreview.net/pdf?id=SkVhlh09tX)|[[slide]](./20190118_jiangtongli.pdf)|-|

### 2019/01/10

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|zhuqile|ICLR2019 [Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow](https://openreview.net/forum?id=HyxPx3R9tm)|[[slide]](./20190110_qilezhu.pdf)|-|
|-|ICLR2017 [Deep Variational Information Bottleneck](https://openreview.net/forum?id=HyxQzBceg)|-|-|
|jiangtongli|COLING2018 [Modeling Multi-turn Conversation with Deep Utterance Aggregation](https://arxiv.org/pdf/1806.09102.pdf)|[[slide](./20190110_jiangtongli.pdf)|-|


### 2018/12/27

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|yxsu|SIGHAN2018 [Group Linguistic Bias Aware Neural Response Generation](http://www.aclweb.org/anthology/W17-6001)|[[slide]](./20181227_yxsu.pdf)|-|
|Shangmingyue|Arxiv2018 [Dialogue Natural Language Inference](https://arxiv.org/pdf/1811.00671.pdf)|[[slide]](./20181227_mingyue.pdf)|-|

### 2018/12/20

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|lixin|EMNLP2018 [Semi-Supervised Learning for Neural Keyphrase Generation](http://aclweb.org/anthology/D18-1447)|[[slide]](./20181220_lixin.pdf)|-|
|gaojun|ACL2018 [Hierarchical Neural Story Generation](http://www.aclweb.org/anthology/P18-1082)|[[slide]](./20181220_gaojun.pdf)|-|

### 2018/12/13

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|gaoyifan|AAAI19 [A Multi-Agent Communication Framework for Question-Worthy Phrase Extraction and Question Generation](http://www.sdspeople.fudan.edu.cn/zywei/paper/2019/wang-aaai-2019.pdf)|[[slide]](./20181213_evanyfgao.pdf)|-|

### 2018/11/29

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|zhufengpan|COLING2016 [Non-sentential Question Resolution using Sequence to Sequence Learning](https://pdfs.semanticscholar.org/269e/d5ba525519502123b58472e069d77c5bda14.pdf)|[[slide]](./20181129_zhufengpan.pdf)|-|
|-|SIGIR2017 [Incomplete Follow-up question Resolution using Retrieval based Sequence to Sequence Learning](https://dl.acm.org/citation.cfm?id=3080801)|-|[[dataset]](https://github.com/vineetm/siri-incomplete-questions)|

### 2018/11/23

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|zhaoyang|ICLR2018(under review)[I Know the Feeling: Learning to Converse with Empathy](https://arxiv.org/pdf/1811.00207.pdf) |[[slide]](./20181123_zhaoyang.pdf)|-|
|jcykcai|NIPS2018 [Deep Generative Models with Learnable Knowledge Constraints](https://arxiv.org/pdf/1806.09764.pdf)|[[slide]](./jcykcai1123.pdf)|-|

### 2018/10/25

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|gaoyifan|ACL2018 [Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia](http://aclweb.org/anthology/P18-1177)|[[slide]](./20181025_evanyfgao.pdf)|-|
|shangmingyue|NIPS2017 [Adversarial Ranking for Language Generation](https://arxiv.org/pdf/1705.11001.pdf)|[[slide]](./RankGAN+LeakGAN.pdf)|-|
|-|AAAI2018 [Long Text Generation via Adversarial Training with Leaked Information](https://arxiv.org/pdf/1709.08624.pdf)|-|-|


### 2018/10/18

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|gaojun|NAACL2017 [Deep contextualized word representations](http://aclweb.org/anthology/N18-1202)|[[slide]](./20181018_gaojun.pdf)|-|
|-|Arxiv2018 [Improving Language Understanding by Generative Pre-Training](https://arxiv.org/pdf/1810.04805.pdf)|-|-|
|-|Arxiv2018 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)|-|-|


### 2018/10/11

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|lixin|ACL2017 [Neural Belief Tracker: Data-Driven Dialogue State Tracking](http://aclweb.org/anthology/P17-1163 )|[[slide]](./20181011_lixin.pdf)|-|
|-|ACL2018 [Global-Locally Self-Attentive Encoder for Dialogue State Tracking](http://aclweb.org/anthology/P18-1135)|-|-|
|-|ICASSP2018 [Adversarial Actor-Critic Model For Task-Completion Dialogue Policy Learning](https://arxiv.org/pdf/1710.11277.pdf)|-|-|
|-|ACL2018 [Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning](http://aclweb.org/anthology/P18-1203)|-|-|

### 2018/9/21

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|cd|NIPS2018 Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization|[[slide]](./20180921_jcykcai.pdf)|-|
||EMNLP2017 Sequential Matching Network-A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots||-|
|zhaoyang|ACL2018 Learning to Control the Specificity in Neural Response Generation|[[slide]](https://github.com/52paper/52paper.github.io/blob/master/zhaoyang_20180801.pdf)|-|

### 2018/8/1

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|gaojun|NIPS2017 [Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space](https://papers.nips.cc/paper/7158-diverse-and-accurate-image-description-using-a-variational-auto-encoder-with-an-additive-gaussian-encoding-space.pdf)|[slide]|-|
|cd|arXiv2018 [Response Generation by Context-aware Prototype Editing](https://arxiv.org/pdf/1806.07042.pdf)|[[slide]](./20180801_jcykcai.pdf)|-|
|-|arXiv2016 [Two are better than one: An ensemble of retrieval-and generation-based dialog systems](https://arxiv.org/pdf/1610.07149.pdf)|-|-|
|zhaoyang|AAAI2018 [Dictionary-Guided Editing Networks for Paraphrase Generation](https://arxiv.org/pdf/1806.08077.pdf)|[[slide]](https://github.com/52paper/52paper.github.io/blob/master/zhaoyang_20180801.pdf)|-|
|ziyang|ACL2018 [Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information](https://arxiv.org/pdf/1805.04655.pdf)|[[slide]](./20180801_willzychen.pdf)|-|
|biwei|-|-|-|
|yahui|ACL2018 [Token-level and sequence-level loss smoothing for RNN language models](https://hal.inria.fr/hal-01790879/file/paper.pdf)|[[slide]](./20180801_amosyhliu.pdf)|-|
|-|arXiv2018 [Sounding Board: A User-Centric and Content-Driven Social Chatbot](https://arxiv.org/pdf/1804.10202.pdf)|-|-|

### 2018/7/27 ACL Report

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|zhaoyang|ACL18 Report|[slide](https://github.com/52paper/52paper.github.io/blob/master/20180727_zhaoyang.pdf)|-|


### 2018/7/18

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|gaojun|ACL2017 [Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning](http://www.nlpr.ia.ac.cn/cip/shizhuhe/articles/acl2017-coreqa.pdf)|[[slide]](./20180718_gaojun.pdf)|-|
|cd|ACL18 [AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples](http://aclweb.org/anthology/P18-1225) |[[slide]](./20180718_jcykcai.pdf)|-|
|-|ACL18 [Working Memory Networks-Augmenting Memory Networks with a Relational Reasoning Module](https://arxiv.org/pdf/1805.09354.pdf) |-|-|
|ziyang|IJCAI2018 [SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks](https://www.ijcai.org/proceedings/2018/0618.pdf)|[[slide]](./20180719_willzychen.pdf)|-|
|biwei|-|-|-|
|yahui|AAAI2015 [Self-Paced Curriculum Learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewFile/9750/9929)|[[slide]](./20180719_amosyhliu.pdf)|-|
|-|ICML2018 [MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels](https://arxiv.org/abs/1712.05055)|-|-|

### 2018/7/3 Reinforcement Learning

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|gaojun|AAAI2018 [Flexible End-to-End Dialogue System for Knowledge Grounded Conversation](https://arxiv.org/pdf/1709.04264.pdf)|[[slide]](./20180703_gaojun.pdf)|-|
|cd|Nature2017 [Mastering the game of Go Without human knowledge](https://www.nature.com/articles/nature24270.pdf)|[[slide]](./20180703_jcykcai.pdf)|-|
|ziyang|CVPR2018 [Video Captioning via Hierarchical Reinforcement Learning](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Captioning_via_CVPR_2018_paper.pdf)|[[slide]](./20190703_willzychen.pdf)|-|
|biwei|ICML2017 [FeUdal Networks for Hierarchical Reinforcement Learning](https://arxiv.org/pdf/1703.01161.pdf)|[[slide]](./20180702_biwei.pdf)|-|
|yahui|IJCAI2018 [Learning to Converse with Noisy Data: Generation with Calibration](https://www.cs.jhu.edu/~npeng/papers/IJCAI18-learning-converse-noisy.pdf)|[[slide]](./20180703_amosyhliu.pdf)|-|
|-|arXiv2016 [Data Distillation for Controlling Specificity in Dialogue Generation](https://arxiv.org/pdf/1702.06703.pdf)|-|-|


### 2018/6/26 GAN review & Knowledge-incoporated Generation & RL

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|xiaojiang|review [questions about GAN](./gan_report.md) again and summarize GAN's possible use in conversation resposne geneartion.|-|-|
|gaojun|IJCAI2016 [Neural Generative Question Answering](https://arxiv.org/pdf/1512.01337.pdf)|[[slide]](./20180626_gaojunv3.pdf)|-|
|-|AAAI2018 [A Knowledge-Grounded Neural Conversation Model](https://arxiv.org/pdf/1702.01932.pdf)|-|-|
|cd|ACL2018 [Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting](https://arxiv.org/pdf/1805.11080.pdf)|[[slide]](./20180625_jcykcai.pdf)|-|
|-|ACL2018 [Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach](https://arxiv.org/pdf/1805.05181.pdf)|-|-|
|yahui|NAACL2018 [Discourse-Aware Neural Rewards for Coherent Text Generation](https://arxiv.org/pdf/1805.03766.pdf)| [[slide]](./20180625_amosyhliu.pdf)|[Report of GAN](https://www.overleaf.com/read/mynrvgxpvvfh)|
|biwei|ICML2017 [Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control](https://arxiv.org/abs/1611.02796)|[[slide]](./20180625_biwei.pdf)|-|


### 2018/6/20 GAN
- xiaojiang's questions, hope we could have agreements on these three points, and output some reports:
  1. Why Seq2seq is better than the previous language model methods in generating language sequence. Why GAN is better than standard Seq2seq?
  2. GAN has been successfully appllied to many new image tasks, such as image generation. What are the best tasks of GAN for text?
  3. Why GAN has no break-through on text yet? All possible reasons.

|Lecturers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|cd|Implement Adversarial Training for Text Generation (motivations and technologies)|[[slide]](./20180621_jcykcai.pdf)|-|
|gaojun|EMNLP2017 [Neural Response Generation via GAN with an Approximate Embedding Layer∗](http://aclweb.org/anthology/D17-1065)|[[slide]](./20180620_gaojun.pdf)|-|
|-|IJCAI2018 [Commonsense Knowledge Aware Conversation Generation with Graph Attention](http://coai.cs.tsinghua.edu.cn/hml/media/files/2018_commonsense_ZhouHao_3_TYVQ7Iq.pdf)|-|-|
|yahui|EMNLP2017 [Adversarial Learning for Neural Dialogue Generation](https://nlp.stanford.edu/pubs/li2017adversarial.pdf)|[[slide]](./20180620_amosyhliu.pdf)|-|
|-|ICLR2018 [MaskGAN: Better Text Generation via Filling in the __ ](https://arxiv.org/pdf/1801.07736.pdf)|-|-|
|biwei|ICML2017 [Adversarial Feature Matching for Text Generation](https://arxiv.org/pdf/1706.03850.pdf)|[[slide]](./20180621_biwei.pdf)|-|
